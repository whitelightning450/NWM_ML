{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:40.666198Z",
     "start_time": "2023-11-09T04:18:40.658467Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "LSTM development script\n"
     ]
    }
   ],
   "source": [
    "# hydrological packages\n",
    "import hydroeval as he\n",
    "from hydrotools.nwm_client import utils \n",
    "\n",
    "# basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import bz2file as bz2\n",
    "\n",
    "# system packages\n",
    "from progressbar import ProgressBar\n",
    "from datetime import datetime, date\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "\n",
    "# data analysi packages\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# deep learning packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#Shared/Utility scripts\n",
    "import sys\n",
    "import boto3\n",
    "import s3fs\n",
    "sys.path.insert(0, '..') #sys allows for the .ipynb file to connect to the shared folder files\n",
    "\n",
    "#load access key\n",
    "HOME = os.path.expanduser('~')\n",
    "KEYPATH = \"NWM_ML/AWSaccessKeys.csv\"\n",
    "ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "#start session\n",
    "SESSION = boto3.Session(\n",
    "    aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "    aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    ")\n",
    "S3 = SESSION.resource('s3')\n",
    "#AWS BUCKET information\n",
    "BUCKET_NAME = 'streamflow-app-data'\n",
    "BUCKET = S3.Bucket(BUCKET_NAME)\n",
    "\n",
    "#s3fs\n",
    "fs = s3fs.S3FileSystem(anon=False, key=ACCESS['Access key ID'][0], secret=ACCESS['Secret access key'][0])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "modelname = \"LSTM\"\n",
    "\n",
    "print(f\"{modelname} development script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "894c64e4a7611ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:43.689720Z",
     "start_time": "2023-11-09T04:18:43.650794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Mean_Basin_Elev_ft</th>\n",
       "      <th>Perc_Forest</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Herbace</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>Mean_Ann_Precip_in</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-28</td>\n",
       "      <td>78.55521</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-29</td>\n",
       "      <td>98.61146</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-30</td>\n",
       "      <td>97.60208</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-31</td>\n",
       "      <td>99.33125</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>95.76354</td>\n",
       "      <td>-0.998630</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id        Lat        Long  Drainage_area_mi2  Mean_Basin_Elev_ft  \\\n",
       "0   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "1   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "2   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "3   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "4   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "\n",
       "   Perc_Forest  Perc_Develop  Perc_Imperv  Perc_Herbace  Perc_Slop_30  \\\n",
       "0         67.7           1.2         0.12          2.94          27.2   \n",
       "1         67.7           1.2         0.12          2.94          27.2   \n",
       "2         67.7           1.2         0.12          2.94          27.2   \n",
       "3         67.7           1.2         0.12          2.94          27.2   \n",
       "4         67.7           1.2         0.12          2.94          27.2   \n",
       "\n",
       "   Mean_Ann_Precip_in    datetime  flow_cfs        s1        s2  storage  swe  \\\n",
       "0                34.8  2010-10-28  78.55521 -0.891007 -0.453991      0.0  1.2   \n",
       "1                34.8  2010-10-29  98.61146 -0.891007 -0.453991      0.0  1.2   \n",
       "2                34.8  2010-10-30  97.60208 -0.891007 -0.453991      0.0  1.1   \n",
       "3                34.8  2010-10-31  99.33125 -0.891007 -0.453991      0.0  1.2   \n",
       "4                34.8  2010-11-01  95.76354 -0.998630  0.052336      0.0  1.2   \n",
       "\n",
       "   NWM_flow  DOY  \n",
       "0      55.0  301  \n",
       "1      55.0  302  \n",
       "2      54.0  303  \n",
       "3      54.0  304  \n",
       "4      54.0  305  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get streamstats data \n",
    "datapath = f\"{HOME}/NWM_ML/Data/input\"\n",
    "file = \"Streamstats.csv\"\n",
    "filepath = f\"{datapath}/{file}\"\n",
    "try:\n",
    "    StreamStats = pd.read_csv(filepath)\n",
    "except:\n",
    "    print(\"Data not found, retreiving from AWS S3\")\n",
    "    if not os.path.exists(datapath):\n",
    "        os.makedirs(datapath, exist_ok=True)\n",
    "    key = 'Streamstats/Streamstats.csv'      \n",
    "    S3.meta.client.download_file(BUCKET_NAME, key,filepath)\n",
    "    StreamStats = pd.read_csv(filepath)\n",
    "\n",
    "#Get processed training data \n",
    "datapath = f\"{HOME}/NWM_ML/Data/Processed\"\n",
    "file = \"raw_training_data.parquet\"\n",
    "filepath = f\"{datapath}/{file}\"\n",
    "try:\n",
    "    raw_training_data = pd.read_parquet(filepath)\n",
    "except:\n",
    "    print(\"Data not found, retreiving from AWS S3\")\n",
    "    if not os.path.exists(datapath):\n",
    "        os.makedirs(datapath, exist_ok=True)\n",
    "    key = \"NWM_ML\"+datapath.split(\"NWM_ML\",1)[1]+'/'+file       \n",
    "    S3.meta.client.download_file(BUCKET_NAME, key,filepath)\n",
    "    raw_training_data = pd.read_parquet(filepath)\n",
    "\n",
    "raw_training_data.pop('Unnamed: 0')\n",
    "raw_training_data['station_id'] = raw_training_data['station_id'].astype('str')\n",
    "raw_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d8ff5-b95a-40ed-af26-9029e1b54947",
   "metadata": {},
   "source": [
    "### Dataprocessing\n",
    "* Editing the features based on the feature importance\n",
    "* Remove headwater stations from dataset\n",
    "* make sure dates are in datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef8b8226-79f9-4f8d-8528-8989050bbdea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128879, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Editing the features based on the feature importance should be done here!!!!!!!!!!!!!!!\n",
    "Training_DF = raw_training_data.copy()\n",
    "Training_DF.drop(['Mean_Ann_Precip_in', 'Perc_Herbace', 'Perc_Forest',\n",
    "                        'Mean_Basin_Elev_ft'], axis=1, inplace=True)\n",
    "\n",
    "#remove headwater stations\n",
    "headwater_stations = ['10011500', '10109000', '10113500', '10128500', '10131000', '10146400', '10150500', '10154200',\n",
    "'10172700', '10172800', '10172952']\n",
    "Training_DF = Training_DF[~raw_training_data['station_id'].isin(headwater_stations)]\n",
    "\n",
    "#convert dates to datetime format\n",
    "Training_DF.datetime = pd.to_datetime(Training_DF.datetime)\n",
    "\n",
    "#Select training data - testing is going to be done on 2020\n",
    "x_train_temp = Training_DF[Training_DF.datetime.dt.year != 2020]\n",
    "x_train_temp.pop('station_id')\n",
    "x_train_temp.pop('datetime')\n",
    "y_train_temp = x_train_temp['flow_cfs']\n",
    "x_train_temp.pop('flow_cfs')\n",
    "\n",
    "#Convert dataframe to numpy, scale, save scalers\n",
    "y_train = y_train_temp.to_numpy()\n",
    "x_train = x_train_temp.to_numpy()\n",
    "\n",
    "scalername_x = \"Area_Perc_Seas_stor_swe_NWM_DOY_scaler_x.save\"\n",
    "scalername_y = \"Area_Perc_Seas_stor_swe_NWM_DOY_scaler_y.save\"\n",
    "modelpath = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "if not os.path.exists(modelpath):\n",
    "    os.makedirs(modelpath, exist_ok=True)\n",
    "\n",
    "scalerfilepath_x = f\"{modelpath}/{scalername_x}\"\n",
    "scalerfilepath_y = f\"{modelpath}/{scalername_y}\"\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "joblib.dump(scaler, scalerfilepath_x)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled_train = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "joblib.dump(scaler, scalerfilepath_y)  \n",
    "y_scaled_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f9cda",
   "metadata": {},
   "source": [
    "### Set up Testing year\n",
    "* Select year(s) not used in training\n",
    "* Convert to numpy array\n",
    "* Load scaler and scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd46d43cce5d1387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:44.100124Z",
     "start_time": "2023-11-10T04:40:44.086373Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get water year for testing from larger dataset\n",
    "x_test_temp = Training_DF[Training_DF.datetime.dt.year == 2020]\n",
    "x_test_temp_1 = x_test_temp.copy()\n",
    "station_index_list = x_test_temp_1['station_id']\n",
    "x_test_temp_1.pop('station_id')\n",
    "x_test_temp_1.pop('datetime')\n",
    "\n",
    "#Get target variable (y) and convert to numpy arrays\n",
    "y_test_temp_1 = x_test_temp_1['flow_cfs']\n",
    "x_test_temp_1.pop('flow_cfs')\n",
    "x_test_1_np = x_test_temp_1.reset_index(drop=True).to_numpy()\n",
    "y_test_1_np = y_test_temp_1.reset_index(drop=True).to_numpy()\n",
    "\n",
    "#load scalers and scale\n",
    "scalername_x = \"Area_Perc_Seas_stor_swe_NWM_DOY_scaler_x.save\"\n",
    "scalername_y = \"Area_Perc_Seas_stor_swe_NWM_DOY_scaler_y.save\"\n",
    "modelpath = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "scalerfilepath_x = f\"{modelpath}/{scalername_x}\"\n",
    "scalerfilepath_y = f\"{modelpath}/{scalername_y}\"\n",
    "\n",
    "#load scalers\n",
    "scaler_x = joblib.load(scalerfilepath_x)\n",
    "scaler_y = joblib.load(scalerfilepath_y)\n",
    "\n",
    "#scale the testing data\n",
    "x_test_1_scaled = scaler_x.fit_transform(x_test_1_np)\n",
    "y_scaled_test_1 = scaler_y.fit_transform(y_test_1_np.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16349b",
   "metadata": {},
   "source": [
    "### Set up model training framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0434049b0d18cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:49.443442Z",
     "start_time": "2023-11-10T04:40:49.440536Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128879, 1, 12])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% LSTM\n",
    "\n",
    "n_targets = 1\n",
    "tries = 10\n",
    "#model performance metrics\n",
    "cri_temp_nse = np.zeros([3, n_targets, tries])\n",
    "cri_temp_rmse = np.zeros([3, n_targets, tries])\n",
    "cri_temp_r2 = np.zeros([3, n_targets, tries])\n",
    "cri_temp_kge = np.zeros([3, n_targets, tries])\n",
    "cri_temp_lognse = np.zeros([3, n_targets, tries])\n",
    "\n",
    "# Convert to tensor for PyTorch, Reshape Input for LSTM Model\n",
    "x_train_scaled_t = torch.Tensor(x_train_scaled).unsqueeze(1)\n",
    "y_train_scaled_t = torch.Tensor(y_scaled_train).unsqueeze(1)\n",
    "#Make sure the tensors on are the respective device (cpu/gpu)\n",
    "x_train_scaled_t = x_train_scaled_t.to(device)\n",
    "y_train_scaled_t = y_train_scaled_t.to(device)\n",
    "\n",
    "x_train_scaled_t.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c944c1e",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "* add lookback\n",
    "* make the model a .py file and class when finalized. PyTorch only saves the weights of the layer/node, not the overall structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "666548fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.009890668032775395\n",
      "Epoch 2/5, Loss: 0.00969704137502152\n",
      "Epoch 3/5, Loss: 0.009692728656836194\n",
      "Epoch 4/5, Loss: 0.009664511577092512\n",
      "Epoch 5/5, Loss: 0.00967886494779579\n",
      "finish\n",
      "Run Time: 0.45086693366368613 minutes \n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "model_path = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming you have your data loaded into NumPy arrays as x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, x_scaled, y_scaled\n",
    "# Hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.0001\n",
    "decay = 1e-2\n",
    "validation_split = 0.2\n",
    "neurons = 300\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(x_train_scaled_t, y_train_scaled_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False ) # might shuffle this\n",
    "\n",
    "# Build the model\n",
    "model = nn.LSTM(input_size=x_train_scaled_t.shape[2], hidden_size=neurons, bidirectional=True, batch_first=True).to(device)\n",
    "fc = nn.Linear(neurons * 2, 1).to(device)  # Multiply by 2 for bidirectional LSTM\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay) #\n",
    "\n",
    "# Training loop \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    fc.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "\n",
    "        output, _ = model(batch_x)\n",
    "        output = fc(output[:, -1, :])\n",
    "        loss = criterion(output, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "print('finish')\n",
    "print(\"Run Time:\" + \" %s minutes \" % ((time.time() - start_time)/60))\n",
    "#save model - https://pytorch.org/tutorials/beginner/saving_loading_models.html, for a more efficient way to save... need to make the model as a class and we save the class...\n",
    "if os.path.exists(model_path) == False:\n",
    "    os.mkdir(model_path)\n",
    "torch.save(model.state_dict(), f\"{model_path}/{modelname}_model.pkl\")\n",
    "torch.save(fc.state_dict(), f\"{model_path}/{modelname}_model_fc.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc059ce0",
   "metadata": {},
   "source": [
    "## Load the model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20bab945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and load the model\n",
    "model_path = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "\n",
    "# Build the model\n",
    "model_P = nn.LSTM(input_size=x_train_scaled_t.shape[2], hidden_size=neurons, bidirectional=True, batch_first=True).to(device)\n",
    "fc_P = nn.Linear(neurons * 2, 1).to(device)  # Multiply by 2 for bidirectional LSTM\n",
    "\n",
    "#this requires the model structure to be preloaded\n",
    "model_P.load_state_dict(torch.load(f\"{model_path}/{modelname}_model.pkl\"))\n",
    "fc_P.load_state_dict(torch.load(f\"{model_path}/{modelname}_model_fc.pkl\"))\n",
    "\n",
    "#put the model scores into a dataframe for comparison\n",
    "#Evaluation columns for prediction time series\n",
    "cols = ['USGSid', 'NHDPlusid', 'NWM_rmse', f\"{modelname}_rmse\", 'NWM_pbias', f\"{modelname}_pbias\", \n",
    "        'NWM_kge', f\"{modelname}__kge\", 'NWM_mape',  f\"{modelname}_mape\"]\n",
    "\n",
    "#Evaluation columns for accumulated supply time series\n",
    "supcols = ['USGSid', 'NHDPlusid', 'NWM_rmse', f\"{modelname}_rmse\", 'NWM_pbias', f\"{modelname}_pbias\", \n",
    "        'NWM_kge', f\"{modelname}__kge\", 'NWM_mape',  f\"{modelname}_mape\", 'Obs_vol', 'NWM_vol', f\"{modelname}_vol\",\n",
    "        'NWM_vol_err', f\"{modelname}_vol_err\", 'NWM_vol_Perc_diff', f\"{modelname}_vol_Perc_diff\"]\n",
    "\n",
    "\n",
    "EvalDF = pd.DataFrame(columns = cols)\n",
    "SupplyEvalDF = pd.DataFrame(columns = supcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1a412",
   "metadata": {},
   "source": [
    "# Make a prediction for each location, save as compressed pkl file, and send predictions to AWS for use in CSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae73c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get annual supply diffs\n",
    "cfsday_AFday = 1.983\n",
    "year = 2020\n",
    "\n",
    "model_P = model_P.to(device)\n",
    "fc_P = fc_P.to(device)\n",
    "\n",
    "\n",
    "Preds_Dict = {}\n",
    "for station_number in station_index_list.drop_duplicates():\n",
    "  index = station_index_list == station_number\n",
    "  X_test = x_test_temp_1[index]\n",
    "  X_test_scaled_t = torch.Tensor(x_test_1_scaled[index]).unsqueeze(1)\n",
    "  X_test_scaled_t = X_test_scaled_t.to(device)\n",
    "  l = len(y_test_temp_1.values)\n",
    "  y_test = torch.Tensor(np.array(y_test_temp_1.values).reshape(l,1))\n",
    "  y_test = y_test.to(device)\n",
    "\n",
    "  # Evaluation\n",
    "  model_P.eval()\n",
    "  with torch.no_grad():\n",
    "    predictions_scaled, _ = model_P(X_test_scaled_t)\n",
    "    predictions_scaled = fc_P(predictions_scaled[:, -1, :])\n",
    "\n",
    "  # Invert scaling for actual\n",
    "  predictions = scaler_y.inverse_transform(predictions_scaled.to('cpu').numpy())\n",
    "  predictions[predictions<0] = 0\n",
    "  predictions = pd.DataFrame(predictions, columns=[f\"{modelname}_flow\"])\n",
    "\n",
    "  #save predictions, need to convert to NHDPlus reach - Need to add Datetime column and flow predictions\n",
    "  #make daterange\n",
    "  dates = pd.date_range(pd.to_datetime(\"2020-01-01\"), periods=len(predictions)).strftime(\"%Y-%m-%d\").tolist()\n",
    "  predictions['Datetime'] = dates\n",
    "    \n",
    "  #get reach id for model eval\n",
    "  nhdreach = utils.crosswalk(usgs_site_codes=station_number)\n",
    "  nhdreach = nhdreach['nwm_feature_id'].iloc[0]\n",
    "\n",
    "  #put columns in correct order\n",
    "  cols = ['Datetime', f\"{modelname}_flow\"]\n",
    "  predictions = predictions[cols]\n",
    "\n",
    "  #save predictions to AWS so we can use CSES\n",
    "  state = StreamStats['state_id'][StreamStats['NWIS_site_id'].astype(str)== station_number].values[0].lower()\n",
    "  csv_key = f\"{modelname}/NHD_segments_{state}.h5/{modelname[:3]}_{nhdreach}.csv\"\n",
    "  predictions.to_csv(f\"s3://{BUCKET_NAME}/{csv_key}\", index = False,  storage_options={'key': ACCESS['Access key ID'][0],\n",
    "                           'secret': ACCESS['Secret access key'][0]})\n",
    "\n",
    "  #Concat DFS and put into dictionary\n",
    "  x_test_temp['nwm_feature_id'] = nhdreach\n",
    "  Dfs = [predictions.reset_index(drop=True),x_test_temp[x_test_temp['station_id']==station_number].reset_index(drop=True)]\n",
    "  Preds_Dict[station_number] = pd.concat(Dfs, axis=1)\n",
    "\n",
    "  #reorganize columns\n",
    "  Preds_Dict[station_number].pop('datetime')\n",
    "  Preds_Dict[station_number].insert(1, f\"{modelname}_flow\", Preds_Dict[station_number].pop(f\"{modelname}_flow\"))\n",
    "  Preds_Dict[station_number].insert(1, \"NWM_flow\", Preds_Dict[station_number].pop(\"NWM_flow\"))\n",
    "  Preds_Dict[station_number].insert(1, \"flow_cfs\", Preds_Dict[station_number].pop(\"flow_cfs\"))\n",
    "  Preds_Dict[station_number].insert(1, \"nwm_feature_id\", Preds_Dict[station_number].pop(\"nwm_feature_id\"))\n",
    "  Preds_Dict[station_number].insert(1, \"station_id\", Preds_Dict[station_number].pop(\"station_id\"))\n",
    "\n",
    "  #push data to AWS so we can use CSES\n",
    "  \n",
    "  \n",
    "#save predictions as compressed pkl file\n",
    "pred_path = f\"{HOME}/NWM_ML/Predictions/Hindcast/{modelname}/{year}\"\n",
    "file_path = f\"{pred_path}/{modelname}_predictions.pkl\"\n",
    "if os.path.exists(pred_path) == False:\n",
    "  os.makedirs(pred_path)\n",
    "\n",
    "with open(file_path, 'wb') as handle:\n",
    "  pkl.dump(Preds_Dict, handle, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e26547c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>station_id</th>\n",
       "      <th>nwm_feature_id</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>LSTM_flow</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>10105900</td>\n",
       "      <td>666170</td>\n",
       "      <td>48.351044</td>\n",
       "      <td>61.0</td>\n",
       "      <td>11.592534</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>10105900</td>\n",
       "      <td>666170</td>\n",
       "      <td>50.033333</td>\n",
       "      <td>61.0</td>\n",
       "      <td>11.503805</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>10105900</td>\n",
       "      <td>666170</td>\n",
       "      <td>48.821877</td>\n",
       "      <td>60.0</td>\n",
       "      <td>11.505720</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>10105900</td>\n",
       "      <td>666170</td>\n",
       "      <td>47.367710</td>\n",
       "      <td>60.0</td>\n",
       "      <td>11.527790</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.30</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>10105900</td>\n",
       "      <td>666170</td>\n",
       "      <td>46.633335</td>\n",
       "      <td>60.0</td>\n",
       "      <td>11.549849</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>2020-09-26</td>\n",
       "      <td>10105900</td>\n",
       "      <td>666170</td>\n",
       "      <td>25.114584</td>\n",
       "      <td>71.0</td>\n",
       "      <td>32.627487</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2020-09-27</td>\n",
       "      <td>10105900</td>\n",
       "      <td>666170</td>\n",
       "      <td>25.642708</td>\n",
       "      <td>71.0</td>\n",
       "      <td>32.649532</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>2020-09-28</td>\n",
       "      <td>10105900</td>\n",
       "      <td>666170</td>\n",
       "      <td>26.336458</td>\n",
       "      <td>71.0</td>\n",
       "      <td>32.671574</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>2020-09-29</td>\n",
       "      <td>10105900</td>\n",
       "      <td>666170</td>\n",
       "      <td>26.250000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>32.689312</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>10105900</td>\n",
       "      <td>666170</td>\n",
       "      <td>27.469791</td>\n",
       "      <td>70.0</td>\n",
       "      <td>32.711361</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>274 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Datetime station_id  nwm_feature_id   flow_cfs  NWM_flow  LSTM_flow  \\\n",
       "0    2020-01-01   10105900          666170  48.351044      61.0  11.592534   \n",
       "1    2020-01-02   10105900          666170  50.033333      61.0  11.503805   \n",
       "2    2020-01-03   10105900          666170  48.821877      60.0  11.505720   \n",
       "3    2020-01-04   10105900          666170  47.367710      60.0  11.527790   \n",
       "4    2020-01-05   10105900          666170  46.633335      60.0  11.549849   \n",
       "..          ...        ...             ...        ...       ...        ...   \n",
       "269  2020-09-26   10105900          666170  25.114584      71.0  32.627487   \n",
       "270  2020-09-27   10105900          666170  25.642708      71.0  32.649532   \n",
       "271  2020-09-28   10105900          666170  26.336458      71.0  32.671574   \n",
       "272  2020-09-29   10105900          666170  26.250000      70.0  32.689312   \n",
       "273  2020-09-30   10105900          666170  27.469791      70.0  32.711361   \n",
       "\n",
       "          Lat       Long  Drainage_area_mi2  Perc_Develop  Perc_Imperv  \\\n",
       "0    41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "1    41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "2    41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "3    41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "4    41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "..        ...        ...                ...           ...          ...   \n",
       "269  41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "270  41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "271  41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "272  41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "273  41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "\n",
       "     Perc_Slop_30        s1        s2  storage   swe  DOY  \n",
       "0            44.2 -0.438371  0.898794      0.0  6.90    1  \n",
       "1            44.2 -0.438371  0.898794      0.0  7.25    2  \n",
       "2            44.2 -0.438371  0.898794      0.0  7.30    3  \n",
       "3            44.2 -0.438371  0.898794      0.0  7.30    4  \n",
       "4            44.2 -0.438371  0.898794      0.0  7.30    5  \n",
       "..            ...       ...       ...      ...   ...  ...  \n",
       "269          44.2 -0.529919 -0.848048      0.0  0.00  270  \n",
       "270          44.2 -0.529919 -0.848048      0.0  0.00  271  \n",
       "271          44.2 -0.529919 -0.848048      0.0  0.00  272  \n",
       "272          44.2 -0.529919 -0.848048      0.0  0.00  273  \n",
       "273          44.2 -0.529919 -0.848048      0.0  0.00  274  \n",
       "\n",
       "[274 rows x 17 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preds_Dict['10105900']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6cc5cb-9850-4535-a180-e0499407265e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3481cdd-b452-42b7-a4bd-c8e4908eab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
