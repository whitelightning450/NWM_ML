{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:40.666198Z",
     "start_time": "2023-11-09T04:18:40.658467Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# hydrological packages\n",
    "import hydroeval as he\n",
    "from hydrotools.nwm_client import utils # I had to pip install this\n",
    "\n",
    "# basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import bz2file as bz2\n",
    "\n",
    "# system packages\n",
    "from progressbar import ProgressBar\n",
    "from datetime import datetime, date\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "\n",
    "# data analysi packages\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# deep learning packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#Shared/Utility scripts\n",
    "import sys\n",
    "import boto3\n",
    "import s3fs\n",
    "sys.path.insert(0, '..') #sys allows for the .ipynb file to connect to the shared folder files\n",
    "\n",
    "#load access key\n",
    "HOME = os.path.expanduser('~')\n",
    "KEYPATH = \"NWM_ML/AWSaccessKeys.csv\"\n",
    "ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "#start session\n",
    "SESSION = boto3.Session(\n",
    "    aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "    aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    ")\n",
    "S3 = SESSION.resource('s3')\n",
    "#AWS BUCKET information\n",
    "BUCKET_NAME = 'streamflow-app-data'\n",
    "BUCKET = S3.Bucket(BUCKET_NAME)\n",
    "\n",
    "#s3fs\n",
    "fs = s3fs.S3FileSystem(anon=False, key=ACCESS['Access key ID'][0], secret=ACCESS['Secret access key'][0])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894c64e4a7611ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:43.689720Z",
     "start_time": "2023-11-09T04:18:43.650794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Mean_Basin_Elev_ft</th>\n",
       "      <th>Perc_Forest</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Herbace</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>Mean_Ann_Precip_in</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-28</td>\n",
       "      <td>78.55521</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-29</td>\n",
       "      <td>98.61146</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-30</td>\n",
       "      <td>97.60208</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-31</td>\n",
       "      <td>99.33125</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>95.76354</td>\n",
       "      <td>-0.998630</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id        Lat        Long  Drainage_area_mi2  Mean_Basin_Elev_ft  \\\n",
       "0   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "1   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "2   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "3   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "4   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "\n",
       "   Perc_Forest  Perc_Develop  Perc_Imperv  Perc_Herbace  Perc_Slop_30  \\\n",
       "0         67.7           1.2         0.12          2.94          27.2   \n",
       "1         67.7           1.2         0.12          2.94          27.2   \n",
       "2         67.7           1.2         0.12          2.94          27.2   \n",
       "3         67.7           1.2         0.12          2.94          27.2   \n",
       "4         67.7           1.2         0.12          2.94          27.2   \n",
       "\n",
       "   Mean_Ann_Precip_in    datetime  flow_cfs        s1        s2  storage  swe  \\\n",
       "0                34.8  2010-10-28  78.55521 -0.891007 -0.453991      0.0  1.2   \n",
       "1                34.8  2010-10-29  98.61146 -0.891007 -0.453991      0.0  1.2   \n",
       "2                34.8  2010-10-30  97.60208 -0.891007 -0.453991      0.0  1.1   \n",
       "3                34.8  2010-10-31  99.33125 -0.891007 -0.453991      0.0  1.2   \n",
       "4                34.8  2010-11-01  95.76354 -0.998630  0.052336      0.0  1.2   \n",
       "\n",
       "   NWM_flow  DOY  \n",
       "0      55.0  301  \n",
       "1      55.0  302  \n",
       "2      54.0  303  \n",
       "3      54.0  304  \n",
       "4      54.0  305  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get streamstats data \n",
    "datapath = f\"{HOME}/NWM_ML/Data/input\"\n",
    "file = \"Streamstats.csv\"\n",
    "filepath = f\"{datapath}/{file}\"\n",
    "try:\n",
    "    StreamStats = pd.read_csv(filepath)\n",
    "except:\n",
    "    print(\"Data not found, retreiving from AWS S3\")\n",
    "    if not os.path.exists(datapath):\n",
    "        os.makedirs(datapath, exist_ok=True)\n",
    "    key = 'Streamstats/Streamstats.csv'      \n",
    "    S3.meta.client.download_file(BUCKET_NAME, key,filepath)\n",
    "    StreamStats = pd.read_csv(filepath)\n",
    "\n",
    "#Get processed training data \n",
    "datapath = f\"{HOME}/NWM_ML/Data/Processed\"\n",
    "file = \"raw_training_data.parquet\"\n",
    "filepath = f\"{datapath}/{file}\"\n",
    "try:\n",
    "    raw_training_data = pd.read_parquet(filepath)\n",
    "except:\n",
    "    print(\"Data not found, retreiving from AWS S3\")\n",
    "    if not os.path.exists(datapath):\n",
    "        os.makedirs(datapath, exist_ok=True)\n",
    "    key = \"NWM_ML\"+datapath.split(\"NWM_ML\",1)[1]+'/'+file       \n",
    "    S3.meta.client.download_file(BUCKET_NAME, key,filepath)\n",
    "    raw_training_data = pd.read_parquet(filepath)\n",
    "\n",
    "raw_training_data.pop('Unnamed: 0')\n",
    "raw_training_data['station_id'] = raw_training_data['station_id'].astype('str')\n",
    "raw_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d8ff5-b95a-40ed-af26-9029e1b54947",
   "metadata": {},
   "source": [
    "### Dataprocessing\n",
    "* Editing the features based on the feature importance\n",
    "* Remove headwater stations from dataset\n",
    "* make sure dates are in datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef8b8226-79f9-4f8d-8528-8989050bbdea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128879, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Editing the features based on the feature importance should be done here!!!!!!!!!!!!!!!\n",
    "Training_DF = raw_training_data.copy()\n",
    "Training_DF.drop(['Mean_Ann_Precip_in', 'Perc_Herbace', 'Perc_Forest',\n",
    "                        'Mean_Basin_Elev_ft'], axis=1, inplace=True)\n",
    "\n",
    "#remove headwater stations\n",
    "headwater_stations = ['10011500', '10109000', '10113500', '10128500', '10131000', '10146400', '10150500', '10154200',\n",
    "'10172700', '10172800', '10172952']\n",
    "Training_DF = Training_DF[~raw_training_data['station_id'].isin(headwater_stations)]\n",
    "\n",
    "#convert dates to datetime format\n",
    "Training_DF.datetime = pd.to_datetime(Training_DF.datetime)\n",
    "\n",
    "#Select training data - testing is going to be done on 2020\n",
    "x_train_temp = Training_DF[Training_DF.datetime.dt.year != 2020]\n",
    "x_train_temp.pop('station_id')\n",
    "x_train_temp.pop('datetime')\n",
    "y_train_temp = x_train_temp['flow_cfs']\n",
    "x_train_temp.pop('flow_cfs')\n",
    "\n",
    "#Convert dataframe to numpy, scale, save scalers\n",
    "y_train = y_train_temp.to_numpy()\n",
    "x_train = x_train_temp.to_numpy()\n",
    "\n",
    "modelname = \"MLP\"\n",
    "scalername_x = \"Area_Perc_Seas_stor_swe_NWM_DOY_scaler_x.save\"\n",
    "scalername_y = \"Area_Perc_Seas_stor_swe_NWM_DOY_scaler_y.save\"\n",
    "modelpath = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "if not os.path.exists(modelpath):\n",
    "    os.makedirs(modelpath, exist_ok=True)\n",
    "\n",
    "scalerfilepath_x = f\"{modelpath}/{scalername_x}\"\n",
    "scalerfilepath_y = f\"{modelpath}/{scalername_y}\"\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "joblib.dump(scaler, scalerfilepath_x)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled_train = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "joblib.dump(scaler, scalerfilepath_y)  \n",
    "y_scaled_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f9cda",
   "metadata": {},
   "source": [
    "### Set up Testing year\n",
    "* Select year(s) not used in training\n",
    "* Convert to numpy array\n",
    "* Load scaler and scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd46d43cce5d1387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:44.100124Z",
     "start_time": "2023-11-10T04:40:44.086373Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get water year for testing from larger dataset\n",
    "x_test_temp = Training_DF[Training_DF.datetime.dt.year == 2020]\n",
    "x_test_temp_1 = x_test_temp.copy()\n",
    "station_index_list = x_test_temp_1['station_id']\n",
    "x_test_temp_1.pop('station_id')\n",
    "x_test_temp_1.pop('datetime')\n",
    "\n",
    "#Get target variable (y) and convert to numpy arrays\n",
    "y_test_temp_1 = x_test_temp_1['flow_cfs']\n",
    "x_test_temp_1.pop('flow_cfs')\n",
    "x_test_1_np = x_test_temp_1.reset_index(drop=True).to_numpy()\n",
    "y_test_1_np = y_test_temp_1.reset_index(drop=True).to_numpy()\n",
    "\n",
    "#load scalers and scale\n",
    "modelname = \"MLP\"\n",
    "scalername_x = \"Area_Perc_Seas_stor_swe_NWM_DOY_scaler_x.save\"\n",
    "scalername_y = \"Area_Perc_Seas_stor_swe_NWM_DOY_scaler_y.save\"\n",
    "modelpath = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "scalerfilepath_x = f\"{modelpath}/{scalername_x}\"\n",
    "scalerfilepath_y = f\"{modelpath}/{scalername_y}\"\n",
    "\n",
    "#load scalers\n",
    "scaler_x = joblib.load(scalerfilepath_x)\n",
    "scaler_y = joblib.load(scalerfilepath_y)\n",
    "\n",
    "#scale the testing data\n",
    "x_test_1_scaled = scaler_x.fit_transform(x_test_1_np)\n",
    "y_scaled_test_1 = scaler_y.fit_transform(y_test_1_np.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16349b",
   "metadata": {},
   "source": [
    "### Set up model training framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0434049b0d18cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:49.443442Z",
     "start_time": "2023-11-10T04:40:49.440536Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% MLP\n",
    "\n",
    "n_targets = 1\n",
    "tries = 10\n",
    "#model performance metrics\n",
    "cri_temp_nse = np.zeros([3, n_targets, tries])\n",
    "cri_temp_rmse = np.zeros([3, n_targets, tries])\n",
    "cri_temp_r2 = np.zeros([3, n_targets, tries])\n",
    "cri_temp_kge = np.zeros([3, n_targets, tries])\n",
    "cri_temp_lognse = np.zeros([3, n_targets, tries])\n",
    "\n",
    "# Convert to tensor for PyTorch\n",
    "x_train_scaled_t = torch.Tensor(x_train_scaled)\n",
    "y_train_scaled_t = torch.Tensor(y_scaled_train)\n",
    "#Make sure the tensors on are the respective device (cpu/gpu)\n",
    "x_train_scaled_t = x_train_scaled_t.to(device)\n",
    "y_train_scaled_t = y_train_scaled_t.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c944c1e",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447219f501a987b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:50:15.960043Z",
     "start_time": "2023-11-10T04:40:59.662734Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0011841386958609057\n",
      "Epoch 2/20, Loss: 0.0009998900827772198\n",
      "Epoch 3/20, Loss: 0.0010072439865052032\n",
      "Epoch 4/20, Loss: 0.0008762492653219075\n",
      "Epoch 5/20, Loss: 0.0007190352959022427\n",
      "Epoch 6/20, Loss: 0.0007379480756709126\n",
      "Epoch 7/20, Loss: 0.0007507959719313107\n",
      "Epoch 8/20, Loss: 0.0005918934312767292\n",
      "Epoch 9/20, Loss: 0.00048261876871947564\n",
      "Epoch 10/20, Loss: 0.00055188945301148\n",
      "Epoch 11/20, Loss: 0.0005203173422703178\n",
      "Epoch 12/20, Loss: 0.0005119359537504274\n",
      "Epoch 13/20, Loss: 0.00047021937113725425\n",
      "Epoch 14/20, Loss: 0.0005001171873563373\n",
      "Epoch 15/20, Loss: 0.0004531849028610537\n",
      "Epoch 16/20, Loss: 0.0004646811413347393\n",
      "Epoch 17/20, Loss: 0.0004428204812767694\n",
      "Epoch 18/20, Loss: 0.00046341442878239473\n",
      "Epoch 19/20, Loss: 0.00041467104534421257\n",
      "Epoch 20/20, Loss: 0.00042162290621680784\n",
      "finish\n",
      "Run Time: 56.165053606033325 seconds \n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "model_path = f\"{HOME}/NWM_ML/Model/MLP\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "decay = 1e-2\n",
    "validation_split = 0.2\n",
    "neurons = 150\n",
    "LD1=128\n",
    "LD2=128\n",
    "LD3=64\n",
    "LD4=64\n",
    "LD5=32\n",
    "LD6=16\n",
    "LD7=5\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(x_train_scaled_t, y_train_scaled_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False )\n",
    "\n",
    "# Build the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(12, LD1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD1, LD2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD2, LD3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD3, LD4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD4, LD5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD5, LD6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD6, 1)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "print('finish')\n",
    "print(\"Run Time:\" + \" %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "#save model\n",
    "if os.path.exists(model_path) == False:\n",
    "    os.mkdir(model_path)\n",
    "torch.save(model.state_dict(), f\"{model_path}/mlp_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc059ce0",
   "metadata": {},
   "source": [
    "## Load the model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20bab945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and load the model\n",
    "model_path = f\"{HOME}/NWM_ML/Model/MLP\"\n",
    "# Hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "decay = 1e-2\n",
    "validation_split = 0.2\n",
    "neurons = 150\n",
    "LD1=128\n",
    "LD2=128\n",
    "LD3=64\n",
    "LD4=64\n",
    "LD5=32\n",
    "LD6=16\n",
    "LD7=5\n",
    "\n",
    "#device = torch.device('cpu') # for some reason had to change to cpu\n",
    "models = nn.Sequential(\n",
    "    nn.Linear(12, LD1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD1, LD2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD2, LD3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD3, LD4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD4, LD5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD5, LD6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD6, 1)\n",
    ").to(device)\n",
    "\n",
    "models.load_state_dict(torch.load(f\"{model_path}/mlp_model.pkl\"))\n",
    "\n",
    "#put the model scores into a dataframe for comparison\n",
    "mod = 'MLP'\n",
    "#Evaluation columns for prediction time series\n",
    "cols = ['USGSid', 'NHDPlusid', 'NWM_rmse', f\"{mod}_rmse\", 'NWM_pbias', f\"{mod}_pbias\", \n",
    "        'NWM_kge', f\"{mod}__kge\", 'NWM_mape',  f\"{mod}_mape\"]\n",
    "\n",
    "#Evaluation columns for accumulated supply time series\n",
    "supcols = ['USGSid', 'NHDPlusid', 'NWM_rmse', f\"{mod}_rmse\", 'NWM_pbias', f\"{mod}_pbias\", \n",
    "        'NWM_kge', f\"{mod}__kge\", 'NWM_mape',  f\"{mod}_mape\", 'Obs_vol', 'NWM_vol', f\"{mod}_vol\",\n",
    "        'NWM_vol_err', f\"{mod}_vol_err\", 'NWM_vol_Perc_diff', f\"{mod}_vol_Perc_diff\"]\n",
    "\n",
    "\n",
    "EvalDF = pd.DataFrame(columns = cols)\n",
    "SupplyEvalDF = pd.DataFrame(columns = supcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8fef2f",
   "metadata": {},
   "source": [
    "# Make a prediction for each location, save as compressed pkl file, and send predictions to AWS for use in CSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c432359e3ae01a43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:50:18.893916Z",
     "start_time": "2023-11-10T04:50:18.778135Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get annual supply diffs\n",
    "cfsday_AFday = 1.983\n",
    "mod = 'MLP'\n",
    "year = 2020\n",
    "\n",
    "\n",
    "Preds_Dict = {}\n",
    "for station_number in station_index_list.drop_duplicates():\n",
    "  #print(station_number)\n",
    "  index = station_index_list == station_number\n",
    "  X_test = x_test_temp_1[index]\n",
    "  X_test_scaled_t = torch.Tensor(x_test_1_scaled[index])\n",
    "  X_test_scaled_t = X_test_scaled_t.to(device)\n",
    "  l = len(y_test_temp_1.values)\n",
    "  y_test = torch.Tensor(np.array(y_test_temp_1.values).reshape(l,1))\n",
    "  y_test = y_test.to(device)\n",
    "\n",
    "  # Evaluation\n",
    "  models.eval()\n",
    "  with torch.no_grad():\n",
    "      predictions_scaled= models(X_test_scaled_t)\n",
    "\n",
    "  # Invert scaling for actual\n",
    "  predictions = scaler_y.inverse_transform(predictions_scaled.to('cpu').numpy())\n",
    "  predictions[predictions<0] = 0\n",
    "\n",
    "  #print('Model Predictions complete')\n",
    "\n",
    "  predictions = pd.DataFrame(predictions, columns=['MLP_flow'])\n",
    "\n",
    "  #save predictions, need to convert to NHDPlus reach - Need to add Datetime column and flow predictions\n",
    "  #make daterange\n",
    "  dates = pd.date_range(pd.to_datetime(\"2020-01-01\"), periods=len(predictions)).strftime(\"%Y-%m-%d\").tolist()\n",
    "  predictions['Datetime'] = dates\n",
    "    \n",
    "  #get reach id for model eval\n",
    "  nhdreach = utils.crosswalk(usgs_site_codes=station_number)\n",
    "  nhdreach = nhdreach['nwm_feature_id'].iloc[0]\n",
    "\n",
    "  #put columns in correct order\n",
    "  cols = ['Datetime', 'MLP_flow']\n",
    "  predictions = predictions[cols]\n",
    "\n",
    "  #save predictions to AWS so we can use CSES\n",
    "  state = StreamStats['state_id'][StreamStats['NWIS_site_id'].astype(str)== station_number].values[0].lower()\n",
    "  csv_key = f\"{modelname}/NHD_segments_{state}.h5/{modelname[:3]}_{nhdreach}.csv\"\n",
    "  predictions.to_csv(f\"s3://{BUCKET_NAME}/{csv_key}\", index = False,  storage_options={'key': ACCESS['Access key ID'][0],\n",
    "                           'secret': ACCESS['Secret access key'][0]})\n",
    "\n",
    "  #Concat DFS and put into dictionary\n",
    "  x_test_temp['nwm_feature_id'] = nhdreach\n",
    "  Dfs = [predictions.reset_index(drop=True),x_test_temp[x_test_temp['station_id']==station_number].reset_index(drop=True)]\n",
    "  Preds_Dict[station_number] = pd.concat(Dfs, axis=1)\n",
    "\n",
    "  #reorganize columns\n",
    "  Preds_Dict[station_number].pop('datetime')\n",
    "  Preds_Dict[station_number].insert(1, \"MLP_flow\", Preds_Dict[station_number].pop(\"MLP_flow\"))\n",
    "  Preds_Dict[station_number].insert(1, \"NWM_flow\", Preds_Dict[station_number].pop(\"NWM_flow\"))\n",
    "  Preds_Dict[station_number].insert(1, \"flow_cfs\", Preds_Dict[station_number].pop(\"flow_cfs\"))\n",
    "  Preds_Dict[station_number].insert(1, \"nwm_feature_id\", Preds_Dict[station_number].pop(\"nwm_feature_id\"))\n",
    "  Preds_Dict[station_number].insert(1, \"station_id\", Preds_Dict[station_number].pop(\"station_id\"))\n",
    "\n",
    "  #push data to AWS so we can use CSES\n",
    "  \n",
    "  \n",
    "#save predictions as compressed pkl file\n",
    "pred_path = f\"{HOME}/NWM_ML/Predictions/Hindcast/{mod}/{year}\"\n",
    "file_path = f\"{pred_path}/{mod}_predictions.pkl\"\n",
    "if os.path.exists(pred_path) == False:\n",
    "    os.mkdir(pred_path)\n",
    "\n",
    "# with bz2.BZ2File(file_path, 'w') as f:\n",
    "#   pkl.dump(Preds_Dict, f)\n",
    "\n",
    "\n",
    "with open(file_path, 'wb') as handle:\n",
    "    pkl.dump(Preds_Dict, handle, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "855ed6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>station_id</th>\n",
       "      <th>nwm_feature_id</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>MLP_flow</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>29.372356</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>29.352655</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>29.287912</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>29.268217</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>29.248516</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>2020-09-19</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>63.0</td>\n",
       "      <td>33.948734</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>2020-09-20</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>33.817249</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>2020-09-21</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.010833</td>\n",
       "      <td>62.0</td>\n",
       "      <td>33.740856</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.017273</td>\n",
       "      <td>62.0</td>\n",
       "      <td>33.664471</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>2020-09-23</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>61.0</td>\n",
       "      <td>33.532970</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Datetime station_id  nwm_feature_id  flow_cfs  NWM_flow   MLP_flow  \\\n",
       "0    2020-01-01   10157500        10375690  0.225000      49.0  29.372356   \n",
       "1    2020-01-02   10157500        10375690  0.270000      49.0  29.352655   \n",
       "2    2020-01-03   10157500        10375690  0.275000      48.0  29.287912   \n",
       "3    2020-01-04   10157500        10375690  0.250000      48.0  29.268217   \n",
       "4    2020-01-05   10157500        10375690  0.285000      48.0  29.248516   \n",
       "..          ...        ...             ...       ...       ...        ...   \n",
       "262  2020-09-19   10157500        10375690  0.010000      63.0  33.948734   \n",
       "263  2020-09-20   10157500        10375690  0.010000      62.0  33.817249   \n",
       "264  2020-09-21   10157500        10375690  0.010833      62.0  33.740856   \n",
       "265  2020-09-22   10157500        10375690  0.017273      62.0  33.664471   \n",
       "266  2020-09-23   10157500        10375690  0.010000      61.0  33.532970   \n",
       "\n",
       "           Lat        Long  Drainage_area_mi2  Perc_Develop  Perc_Imperv  \\\n",
       "0    40.460789 -111.472687               49.8          2.37         0.16   \n",
       "1    40.460789 -111.472687               49.8          2.37         0.16   \n",
       "2    40.460789 -111.472687               49.8          2.37         0.16   \n",
       "3    40.460789 -111.472687               49.8          2.37         0.16   \n",
       "4    40.460789 -111.472687               49.8          2.37         0.16   \n",
       "..         ...         ...                ...           ...          ...   \n",
       "262  40.460789 -111.472687               49.8          2.37         0.16   \n",
       "263  40.460789 -111.472687               49.8          2.37         0.16   \n",
       "264  40.460789 -111.472687               49.8          2.37         0.16   \n",
       "265  40.460789 -111.472687               49.8          2.37         0.16   \n",
       "266  40.460789 -111.472687               49.8          2.37         0.16   \n",
       "\n",
       "     Perc_Slop_30        s1        s2  storage  swe  DOY  \n",
       "0            49.1 -0.438371  0.898794      0.0  0.0    1  \n",
       "1            49.1 -0.438371  0.898794      0.0  0.0    2  \n",
       "2            49.1 -0.438371  0.898794      0.0  0.0    3  \n",
       "3            49.1 -0.438371  0.898794      0.0  0.0    4  \n",
       "4            49.1 -0.438371  0.898794      0.0  0.0    5  \n",
       "..            ...       ...       ...      ...  ...  ...  \n",
       "262          49.1 -0.529919 -0.848048      0.0  0.0  270  \n",
       "263          49.1 -0.529919 -0.848048      0.0  0.0  271  \n",
       "264          49.1 -0.529919 -0.848048      0.0  0.0  272  \n",
       "265          49.1 -0.529919 -0.848048      0.0  0.0  273  \n",
       "266          49.1 -0.529919 -0.848048      0.0  0.0  274  \n",
       "\n",
       "[267 rows x 17 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preds_Dict['10157500']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339fa48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294d169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7797c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21cdd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#       # merge\n",
    "#     Eval_DF_mine = pd.concat(Dfs, axis=1)\n",
    "#     prediction_columns = [ 'NWM_flow', 'MLP_flow']\n",
    "\n",
    "#     #plot the predictions\n",
    "#    # Model_Evaluation_Plots(Eval_DF_mine, prediction_columns)\n",
    "\n",
    "#     #Get RMSE from the model\n",
    "#     rmse = RMSE(Eval_DF_mine, prediction_columns)\n",
    "\n",
    "#     #Get Mean Absolute Percentage Error from the model\n",
    "#     mape = MAPE(Eval_DF_mine, prediction_columns)\n",
    "\n",
    "#     #Get Percent Bias from the model\n",
    "#     pbias = PBias(Eval_DF_mine, prediction_columns)\n",
    "\n",
    "#     #Get Kling-Gutz Efficiency from the model\n",
    "#     kge = KGE(Eval_DF_mine, prediction_columns)\n",
    "    \n",
    "#       #Get Volumetric values\n",
    "#     Eval_DF_mine['Datetime'] = pd.to_datetime(dates)\n",
    "#     Eval_DF_mine.set_index('Datetime', inplace = True, drop =True)\n",
    "#     flowcols = [f\"{mod}_flow\", 'flow_cfs', 'NWM_flow']\n",
    "#     SupplyEval = Eval_DF_mine[flowcols].copy()\n",
    "#     SupplyEval = SupplyEval*cfsday_AFday\n",
    "#     #set up cumulative monthly values\n",
    "#     SupplyEval['Year'] = SupplyEval.index.year\n",
    "\n",
    "#     for site in flowcols:\n",
    "#         SupplyEval[site] = SupplyEval.groupby(['Year'])[site].cumsum()  \n",
    "\n",
    "#     EOY_mod_vol_af = SupplyEval[f\"{mod}_flow\"].iloc[-1]\n",
    "#     EOY_obs_vol_af = SupplyEval[\"flow_cfs\"].iloc[-1]\n",
    "#     EOY_nwm_vol_af = SupplyEval[f\"NWM_flow\"].iloc[-1]\n",
    "#     NWM_vol_diff_af = EOY_nwm_vol_af - EOY_obs_vol_af\n",
    "#     Mod_vol_diff_af = EOY_mod_vol_af - EOY_obs_vol_af\n",
    "#     NWM_Perc_diff = (NWM_vol_diff_af/EOY_obs_vol_af)*100\n",
    "#     Mod_Perc_diff = (Mod_vol_diff_af/EOY_obs_vol_af)*100\n",
    "    \n",
    "#      #Get Performance Metrics from the model\n",
    "#     Srmse = RMSE(SupplyEval, prediction_columns)\n",
    "#     Smape = MAPE(SupplyEval, prediction_columns)\n",
    "#     Spbias = PBias(SupplyEval, prediction_columns)\n",
    "#     Skge = KGE(SupplyEval, prediction_columns)\n",
    "    \n",
    "    \n",
    "#     #save model performance\n",
    "#     sitestats = [station_number, nhdreach, rmse[0], rmse[1],  pbias[0], pbias[1], kge[0], kge[1], mape[0],mape[1]]\n",
    "#     EvalDF.loc[len(EvalDF)] = sitestats\n",
    "    \n",
    "#     Supplystats = [station_number, nhdreach, Srmse[0], Srmse[1],  Spbias[0], Spbias[1], Skge[0], Skge[1], Smape[0],  \n",
    "#                  Smape[1],EOY_obs_vol_af, EOY_nwm_vol_af,EOY_mod_vol_af,NWM_vol_diff_af,Mod_vol_diff_af, NWM_Perc_diff, Mod_Perc_diff ]\n",
    "#     SupplyEvalDF.loc[len(SupplyEvalDF)] = Supplystats\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     #put prediction DF into dictionary\n",
    "#     Eval_DF_mine.sort_values(by=['DOY'], inplace=True)\n",
    "#     Eval_DF_mine['Datetime'] = pd.to_datetime(dates)\n",
    "#     Eval_DF_mine.set_index('Datetime', inplace = True)\n",
    "#     SitesDict[nhdreach] = Eval_DF_mine\n",
    "    \n",
    "# #save model results\n",
    "# EvalDF.to_csv(f\"./Predictions/Hindcast/{mod}/{mod}_Performance.csv\")   \n",
    "# SupplyEvalDF.to_csv(f\"./Predictions/Hindcast/{mod}/{mod}_Supply_Performance.csv\")\n",
    "\n",
    "\n",
    "# print(\"Model Performance for Daily cfs\")\n",
    "# display(EvalDF)   \n",
    "# print(\"Model Performance for Daily Accumulated Supply (Acre-Feet)\")\n",
    "# display(SupplyEvalDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da49b51-1d1e-4a4d-9e16-f7a7193b2296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save model results\n",
    "EvalDF.to_csv(f\"./Predictions/Hindcast/{mod}/{mod}_Performance.csv\")\n",
    "\n",
    "for site in SitesDict.keys():\n",
    "    SitesDict[site].rename(columns = {'flow_cfs':'Obs_flow'}, inplace =  True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdaf3c-c4ca-4b3b-93bb-a4ec29814d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "save_path = f\"{HOME}/NWM-ML/Predictions/Hindcast/{mod}\"\n",
    "with open(f'{save_path}/Pred_dict.pkl', 'wb') as handle:\n",
    "    pkl.dump(SitesDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d33a5-56a2-4bfd-90ff-afe319b0130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = 'MLP'\n",
    "save_path = f\"{HOME}/NWM-ML/Predictions/Hindcast/{mod}\"\n",
    "with open(f'{save_path}/Pred_dict.pkl', 'rb') as handle:\n",
    "    SitesDict = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8ebd4-8ffc-4e0d-8dae-170acfc579ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(FigureGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb2a06-96bb-4a84-a2e8-b70753a60c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import FigureGenerator\n",
    "\n",
    "model = 'MLP'\n",
    "plotname = 'MLP_TS_plot'\n",
    "freq = 'D'\n",
    "supply = True\n",
    "fill_between = True\n",
    "title = 'Observed and Modeled flows for NHDPlus Reaches \\n with Upstream Reservoirs in the Great Salt Lake Basin'\n",
    "FigureGenerator.TS_plot(SitesDict, model, plotname, title, freq, supply, fill_between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7778762-427f-44ef-bdc9-eb7df4d6089c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotname = 'MLP_ParityPlot'\n",
    "FigureGenerator.Parity_plot(SitesDict, model, plotname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d175d2-2215-42a9-a8f4-6362891849dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import AWS_transfer\n",
    "model = 'MLP'\n",
    "AWS_transfer.Predictions2AWS(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6cc5cb-9850-4535-a180-e0499407265e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3481cdd-b452-42b7-a4bd-c8e4908eab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
